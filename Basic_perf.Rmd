---
title: "Performance in R 4.5.1 using microbenchmark"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    theme: 
      # version: 4
      # bootswatch: darkly
      version: 5
      bootswatch: vapor
---

```{r setup, include = FALSE}
# options(warn = -1)
suppressMessages({
  library(data.table)
  library(dplyr)
  library(dtplyr)
  library(microbenchmark)
  library(glue)
  library(plotly)
  library(tidyr)
  library(bslib)
  library(parallelly)
  library(parallel)
  library(stringr)
  library(duckdb)
  library(arrow)
})

source("./other_functions.R")
setDTthreads(threads = 4)
knitr::opts_chunk$set(echo = TRUE)
```
# Basic R
## Cast dates
```{r, echo = TRUE}
start_date <- "2017-01-01"
end_date <- "2019-12-31"
f1<-function(d2, d1){
  n_weeks <-  floor(as.numeric(difftime(d2, d1, units="weeks")))
}
f2<-function(d2, d1){
  n_weeks <- floor(as.numeric(difftime(as.Date(d2)
    , as.Date(d1), units = "weeks")))
}
m1<-microbenchmark(
  Nocast = f1(end_date, start_date),
  Cast = f2(end_date, start_date),
  times = 1000
)
print(m1)
```

```{r, warning = FALSE}
fbox_plot(m1, "microseconds")
```

## Explicit vector length vector("type", length) is faster than an empty vector c()
```{r, echo = TRUE}
no_size <- function (n){
  x <- c()
  for (i in seq(n)) {
    x <- c(x, i)
  }
}
explicit_size <- function (n){
  x <- vector("integer", n)
  for (i in seq(n)) {
    x[i] <- i
  }
}
m3 <- microbenchmark(
  no_size = no_size(1e4),
  explicit_size = explicit_size(1e4),
  times = 10
)
print(m3)
```

```{r, warning = FALSE}
fbox_plot(m3, "microseconds")
```

## which function is slow for some simple situations

```{r, echo = TRUE}
vector <- runif(1e8)
w1 <- function(x){
  d <- length(which(x > .5))
}
w2 <- function(x){
  d <- sum(x > .5)
}

m4 <- microbenchmark(
  which = w1(vector),
  nowhich = w2(vector),
  times = 10
)
print(m4)
```

```{r, warning = FALSE}
fbox_plot(m4, "miliseconds")
```

## Column operation is faster than row operation

```{r, echo = TRUE}
n <- 1e4
dt <- data.table(
  a = seq(n), b = runif(n)
)
v1 <- function(dt){
  d <- mean(dt[dt$b > .5, ]$a)
}
v2 <- function(dt){
  d <- mean(dt$a[dt$b > .5])
}
m5 <- microbenchmark(
  row_operation = v1(dt),
  column_operation = v2(dt),
  times = 10
)
print(m5)
```

```{r, warning = FALSE}
fbox_plot(m5, "microseconds")
```

## Sequences function safer than 1:n

The function seq prevents when the second part of the 1:x is zero

```{r, echo = TRUE}
num <- 1e7
s1 <- function(num){
  d <- mean(1:num)
}
s2 <- function(num){
  d <- mean(seq(num))
}
m6<-microbenchmark(
  noseq = s1(num),
  seq = s2(num),
  times = 30
)
print(m6)
```

```{r, warning = FALSE}
fbox_plot(m6, "miliseconds")
```

## paste0 is faster than glue

```{r, echo = TRUE}
large_dataset <- data.table(
  id = 1:1000000,
  value = sample(letters, 1000000, replace = TRUE)
)
a1 <- function(x){
  d <- x |> mutate(code = paste0(id, "_", value))
}
a2 <- function(x){
  d <- x |> mutate(code = glue("{id}_{value}"))
}
m7 <- microbenchmark(
  with_paste = a1(large_dataset),
  with_glue = a2(large_dataset),
  times = 20
)
print(m7)
```

```{r, warning = FALSE}
fbox_plot(m7, "miliseconds")
```

## for loop vs lapply

```{r, echo = TRUE}

# Create a large list
big_list <- replicate(1e5, rnorm(10), simplify = FALSE)

m8 <- microbenchmark(
  lapply = lapply(big_list, mean),
  for_loop = {
    result <- list()
    for (i in seq_along(big_list)) {
      result[[i]] <- mean(big_list[[i]])
    }
  },
  times = 10
)

print(m8)
```

```{r, warning = FALSE}
fbox_plot(m8, "miliseconds")
```
# data.table package functions
## Date vs IDate

```{r, echo = TRUE}
dt <- data.table(
  Date = as.Date('2023-01-01') + 0:99999,
  iDate = as.IDate('2023-01-01') + 0:99999,
  Value = rnorm(100000)
)

nd1 <- as.Date('2023-01-01')
nd2 <- as.Date('2023-01-10')
id1 <- as.IDate('2023-01-01')
id2 <- as.IDate('2023-01-10')

date_between_operation <- function(nd1, nd2) {
  dt |> filter(Date >= nd1 & Date <= nd2)
}
idate_between_operation <- function(id1, id2) {
  dt |> _[data.table::between(iDate, id1, id2)]
}

m9 <- microbenchmark(
  Date = date_between_operation(nd1, nd2),
  iDate = idate_between_operation(id1, id2),
  times = 200L
)
print(m9)
```

```{r, warning = FALSE}
fbox_plot(m9, "miliseconds")
```

## Base R switch vs Dplyr case_when (for simple tasks)

```{r, echo = TRUE}
switch_function <- function(x) {
  switch(x,
         "a" = "apple",
         "b" = "banana",
         "c" = "cherry",
         "default")
}
case_when_function <- function(x) {
  case_when(
    x == "a" ~ "apple",
    x == "b" ~ "banana",
    x == "c" ~ "cherry",
    TRUE ~ "default"
  )
}
# Create a vector of test values
test_values <- sample(c("a", "b", "c", "d"), 1000, replace = TRUE)
m10 <- microbenchmark(
  switch = sapply(test_values, switch_function),
  case_when = sapply(test_values, case_when_function),
  times = 200L
)
print(m10)
```

```{r, warning = FALSE}
fbox_plot(m10, "microseconds")
```

## data.table fcase vs Dplyr case_when

```{r, echo = TRUE}
set.seed(123)
n <- 1e6
data <- data.table(
  id = seq(n),
  value = sample(seq(100), n, replace = TRUE)
)

casewhenf <- function(data){
  df <- data |> 
    mutate(category = case_when(
      value <= 20 ~ "Low",
      value <= 70 ~ "Medium",
      value > 70 ~ "High"))
}
fcasef <- function(data){
  df <- data |> 
    mutate(category = fcase(
      value <= 20, "Low",
      value <= 70, "Medium",
      value > 70, "High"))
}
m11 <- microbenchmark(
  case_when = casewhenf(data),
  fcase = fcasef(data),
  times = 20
)
print(m11)
```

```{r, warning = FALSE}
fbox_plot(m11, "miliseconds")
```

## data.table fcoalesce vs tidyr replace_na

```{r, echo = TRUE}
set.seed(123)
DT <- data.table(
  ID = 1:1e6,
  Value1 = sample(c(NA, 1:100), 1e6, replace = TRUE),
  Value2 = sample(c(NA, 101:200), 1e6, replace = TRUE)
)

# Define the functions
replace_na_f <- function(data){
  DF <- data |> 
    mutate(Value1 = replace_na(Value1, 0),
           Value2 = replace_na(Value2, 0)) |> 
    as.data.table()
}
fcoalesce_f <- function(data){
  DF <- data |> 
    mutate(Value1 = fcoalesce(Value1, 0L),
           Value2 = fcoalesce(Value2, 0L))
}
m12 <- microbenchmark(
  treplace_na = replace_na_f(DT),
  tfcoalesce = fcoalesce_f(DT),
  times = 20
)
print(m12)
```

```{r, warning = FALSE}
fbox_plot(m12, "miliseconds")
```

## data.table notation vs dplyr notation

```{r, echo = TRUE}
dt <- data.table(field_name = c("argentina.blue.man.watch", 
                                "brazil.red.woman.shoes", 
                                "canada.green.kid.hat", 
                                "denmark.red.man.shirt"))

# Filter rows where 'field_name' does not contain 'red'
dtnot <- function(data){
  filtered_dt <- data |> _[!grepl("red", field_name)]
}
anonymousnot <- function(data){
  filtered_dt <- data |> (\(dt) dt[!grepl("red", dt$field_name), ])()
}
dplyrnot <- function(data){
  filtered_dt <- data |> filter(!grepl("red", field_name))
}

m13 <- microbenchmark(
  anonymous_not = anonymousnot(dt),
  data_table_not = dtnot(dt),
  dplyr_not = dplyrnot(dt),
  times = 100
)
print(m13)
```

```{r, warning = FALSE}
fbox_plot(m13, "microseconds")
```

## data.table melt vs tidyr pivot_longer

```{r, echo = TRUE}
large_data <- data.table(
  id = 1:100000,
  var1 = rnorm(100000),
  var2 = rnorm(100000),
  var3 = rnorm(100000),
  var4 = rnorm(100000)
)
# Benchmarking
m14 <- microbenchmark(
  tidyr_pivot_longer = {
    long_data_tidyr <- pivot_longer(large_data, cols = starts_with("var"), 
                                    names_to = "variable", values_to = "value")
  },
  data_table_melt = {
    long_data_dt <- melt(large_data, id.vars = "id", variable.name = "variable", 
                         value.name = "value")
  },
  times = 10
)

print(m14)
```

```{r, warning = FALSE}
fbox_plot(m14, "microseconds")
```

## data.table CJ vs tidyr expand_grid

```{r, echo = TRUE}
vec1 <- seq(1000)
vec2 <- seq(1000)

# Define functions to be benchmarked
expand_grid_func <- function() {
  return(expand_grid(vec1, vec2))
}

CJ_func <- function() {
  return(CJ(vec1, vec2))
}

# Perform benchmarking
m15 <- microbenchmark(
  expand_grid = expand_grid_func(),
  CJ = CJ_func(),
  times = 10
)

print(m15)
```

```{r, warning = FALSE}
fbox_plot(m15, "microseconds")
```

## data.table rbindlist vs R rbind

```{r, echo = TRUE}
# Sample data
size = 1e4
set.seed(44)
df_list <- replicate(50, data.table(id = sample(seq(size), size, replace = T),
                                    value = rnorm(size)), simplify = F)

simple_bind <- function(list_of_dfs){
  do.call(rbind, list_of_dfs)
}

dplyr_bind <- function(list_of_dfs){
  bind_rows(list_of_dfs)
}

dt_bind <- function(list_of_dfs){
  rbindlist(list_of_dfs, fill = F)
}

# Benchmark both methods
m16 <- microbenchmark(
  dt_ver = dt_bind(df_list),
  simple = simple_bind(df_list),
  dplyr_ver = dplyr_bind(df_list),
  times = 30
)

print(m16)
```

```{r, warning = FALSE}
fbox_plot(m16, "microseconds")
```

## stringr word vs tidyr separate vs data.table tstrsplit

```{r, echo = TRUE}
set.seed(123)
n <- 1e4
df <- data.table(text = paste("word1", "word2", "word3", "word4", "word5", sep = "."), stringsAsFactors = F)
df <- df[rep(1, n), , drop = F]

# Using tidyr::separate
separate_words <- function() {
  df |> 
    separate(text, into = c("w1", "w2", "w3", "w4", "w5"), sep = "\\.", remove = F) |> 
    select(-c(w1, w2, w4))
}

# Using stringr::word
stringr_words <- function() {
  df |> 
    mutate(
      w3 = word(text, 3, sep = fixed(".")),
      w5 = word(text, 5, sep = fixed("."))
    )
}

datatable_words <- function() {
  df |> _[, c("w3", "w5") := tstrsplit(text, "\\.")[c(3, 5)]]
}

m17 <- microbenchmark(
  separate = separate_words(),
  stringr = stringr_words(),
  dt = datatable_words(),
  times = 10
)

print(m17)
```

```{r, warning = FALSE}
fbox_plot(m17, "miliseconds")
```

## data.table na_omit vs dplyr drop_na

```{r, echo = TRUE}
# Sample data
set.seed(123)
n <- 1e6
df <- data.table(
  x = rnorm(n),
  y = sample(c(NA, 1:100), n, replace = TRUE),
  z = sample(c(NA, letters), n, replace = TRUE),
  stringsAsFactors = F
)

# Benchmark both methods
m18 <- microbenchmark(
  dplyr_drop_na = {
    df |> drop_na()
  },
  data_table_na_omit = {
    dt |> na.omit()
  },
  times = 10
)

print(m18)
```

```{r, warning = FALSE}
fbox_plot(m18, "microseconds")
```
# Parallel processing
## lapply vs parallel mclapply

```{r, echo = TRUE}
# Sample data
set.seed(123)

size = 1e4
n_cores = parallelly::availableCores()

df_list <- replicate(100, data.table(id = sample(seq(size), size, replace = T),
                                    value = rnorm(size)), simplify = F)
extra_df <- data.table(id = sample(seq(size), size, replace = T), 
                       extra_value = runif(size))

# Sequential join
sequential_join <- function() {
  lapply(df_list, function(df) {
    merge(df, extra_df, by = "id", allow.cartesian = T)
  })
}

# Parallel join using mclapply
parallel_join <- function() {
  mclapply(df_list, function(df) {
    merge(df, extra_df, by = "id", allow.cartesian = T)
  }, mc.cores = n_cores, mc.silent = T, mc.cleanup = T)
}

# Benchmark both methods
m19 <- microbenchmark(
  sequential = sequential_join(),
  parallel = parallel_join(),
  times = 10
)

print(m19)
```

```{r, warning = FALSE}
fbox_plot(m19, "miliseconds")
```
# dtplyr
This is another alternative (You need to install this package)

```{r, echo = TRUE}
set.seed(123)
n <- 1e7
df <- data.table(
  group1 = sample(LETTERS[1:10], n, replace = TRUE),
  group2 = sample(letters[1:5], n, replace = TRUE),
  value1 = rnorm(n),
  value2 = runif(n, 1, 100)
)

m21 <- microbenchmark(
  basic_way = {
    dplyr <- df |> 
      filter(value1 > 0) |> 
      mutate(ratio = value1 / value2) |> 
      summarize(
        mean_val1 = mean(value1),
        sd_val1 = sd(value1),
        median_val2 = median(value2),
        max_ratio = max(ratio), .by = c("group1", "group2")) |> 
      as.data.table()
  },
  dtplyr_way = {
    dtplyr = df |> 
      lazy_dt() |> 
      filter(value1 > 0) |> 
      mutate(ratio = value1 / value2) |> 
      summarize(
        mean_val1 = mean(value1),
        sd_val1 = sd(value1),
        median_val2 = median(value2),
        max_ratio = max(ratio), .by = c("group1", "group2")) |> 
      as.data.table()
  },
  times = 5
)

print(m21)
```

```{r, warning = FALSE}
fbox_plot(m21, "miliseconds")
```

# Duckdb
## Dockdb files vs parquet
Parquet files may take longer if you partitioned by day. Consider to try partitioning by year or try to use duckdb. Another advantage using duckdb is memory consumption since you can wrangle using SQL statement.

Note: DuckDB requires to open a connection, consider the parameter read_only if you only want to get data. Don't forget to close the connection.
```{r, warning = FALSE}
# partitioned_by_day <- "/conf/posit_azure_logs/data/merge_uip_data_test"
partitioned_by_year <- "/conf/posit_azure_logs/test_290825/data/merge_uip_data_test"
my_duckdb <- "/conf/posit_azure_logs/test_290825/data/my_db.duckdb"

with_parquet <- function(folder_path){
  data_1 <- open_dataset(file.path(folder_path)) |>
    select(
      ALL_WIP_CP_day_session, ALL_WIP_CP_night_session,
      ALL_WIP_BP_day_session, ALL_WIP_BP_night_session,
      ALL_WIP_CP_DS_mem_limit, ALL_WIP_CP_NS_mem_limit,
      ALL_WIP_BP_DS_mem_limit, ALL_WIP_BP_NS_mem_limit,
      ALL_WIP_CP_DS_mem_request, ALL_WIP_CP_NS_mem_request,
      ALL_WIP_BP_DS_mem_request, ALL_WIP_BP_NS_mem_request,
      ALL_WIP_CP_DS_mem_max, ALL_WIP_CP_NS_mem_max,
      ALL_WIP_BP_DS_mem_max, ALL_WIP_BP_NS_mem_max,
      ALL_WIP_CP_node_total, ALL_WIP_BP_node_total
    ) |>
    mutate(
      computepool_node_mem = ALL_WIP_CP_node_total * (160 * 1024),
      bigpool_node_mem = ALL_WIP_BP_node_total * (256 * 1024),
      ALL_WIP_day_session = ALL_WIP_CP_day_session + ALL_WIP_BP_day_session,
      ALL_WIP_night_session = ALL_WIP_CP_night_session + ALL_WIP_BP_night_session,
      ALL_WIP_node_total = ALL_WIP_CP_node_total + ALL_WIP_BP_node_total,
      total_mem_limit = ALL_WIP_CP_DS_mem_limit + ALL_WIP_CP_NS_mem_limit + ALL_WIP_BP_DS_mem_limit + ALL_WIP_BP_NS_mem_limit,
      total_mem_request = ALL_WIP_CP_DS_mem_request + ALL_WIP_CP_NS_mem_request + ALL_WIP_BP_DS_mem_request + ALL_WIP_BP_NS_mem_request,
      total_mem_max = ALL_WIP_CP_DS_mem_max + ALL_WIP_CP_NS_mem_max + ALL_WIP_BP_DS_mem_max + ALL_WIP_BP_NS_mem_max,
      total_node_mem = computepool_node_mem + bigpool_node_mem,
      average_session_per_node = ifelse(ALL_WIP_node_total != 0,
                                         (ALL_WIP_day_session + ALL_WIP_night_session) / ALL_WIP_node_total, 0)
    ) |>
    arrange(ALL_WIP_CP_day_session, ALL_WIP_CP_night_session, ALL_WIP_BP_day_session) |> 
    collect() |>
    as.data.table()
}

with_duckfile <- function(my_path){
  my_connection = dbConnect(duckdb::duckdb(), dbdir = my_path, read_only=TRUE)
  data_2 <- res_duckdb_sql <- dbGetQuery(
    my_connection,
    statement = "select
        ALL_WIP_CP_day_session, ALL_WIP_CP_night_session,
        ALL_WIP_BP_day_session, ALL_WIP_BP_night_session,
        ALL_WIP_CP_DS_mem_limit, ALL_WIP_CP_NS_mem_limit,
        ALL_WIP_BP_DS_mem_limit, ALL_WIP_BP_NS_mem_limit,
        ALL_WIP_CP_DS_mem_request, ALL_WIP_CP_NS_mem_request,
        ALL_WIP_BP_DS_mem_request, ALL_WIP_BP_NS_mem_request,
        ALL_WIP_CP_DS_mem_max, ALL_WIP_CP_NS_mem_max,
        ALL_WIP_BP_DS_mem_max, ALL_WIP_BP_NS_mem_max,
        ALL_WIP_CP_node_total, ALL_WIP_BP_node_total,
        ALL_WIP_CP_node_total * 160 * 1024 as computepool_node_mem,
        ALL_WIP_BP_node_total * 256 * 1024 as bigpool_node_mem,
        ALL_WIP_CP_day_session + ALL_WIP_BP_day_session as ALL_WIP_day_session,
        ALL_WIP_CP_night_session + ALL_WIP_BP_night_session as ALL_WIP_night_session,
        ALL_WIP_CP_node_total + ALL_WIP_BP_node_total as ALL_WIP_node_total,
        ALL_WIP_CP_DS_mem_limit + ALL_WIP_CP_NS_mem_limit + ALL_WIP_BP_DS_mem_limit + ALL_WIP_BP_NS_mem_limit as total_mem_limit,
        ALL_WIP_CP_DS_mem_request + ALL_WIP_CP_NS_mem_request + ALL_WIP_BP_DS_mem_request + ALL_WIP_BP_NS_mem_request as total_mem_request,
        ALL_WIP_CP_DS_mem_max + ALL_WIP_CP_NS_mem_max + ALL_WIP_BP_DS_mem_max + ALL_WIP_BP_NS_mem_max as total_mem_max,
        computepool_node_mem + bigpool_node_mem as total_node_mem,
        CASE 
          WHEN ALL_WIP_node_total != 0 THEN (ALL_WIP_day_session + ALL_WIP_night_session) / ALL_WIP_node_total
          ELSE 0
        END AS average_session_per_node
      from mytable order by ALL_WIP_CP_day_session, ALL_WIP_CP_night_session, ALL_WIP_BP_day_session",
    immediate = TRUE) |> 
    as.data.table()
  
  dbDisconnect(my_connection, shutdown = TRUE)
}

m22 <- microbenchmark(
  duckdb_file = with_duckfile(my_duckdb),
  parquet_by_year = with_parquet(partitioned_by_year),
  times = 2
)

print(m22)
```

```{r, warning = FALSE}
fbox_plot(m22, "miliseconds")
```
