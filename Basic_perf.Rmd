---
title: "Performance in R using microbenchmark"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    theme: 
      # version: 4
      # bootswatch: darkly
      version: 5
      bootswatch: vapor
---

```{r setup, include = FALSE}
# options(warn = -1)
suppressMessages({
  library(data.table)
  library(dplyr)
  library(dtplyr)
  library(microbenchmark)
  library(glue)
  library(plotly)
  library(tidyr)
  library(bslib)
  library(parallelly)
  library(parallel)
  library(stringr)
  library(duckdb)
  library(arrow)
})

source("./other_functions.R")
setDTthreads(threads = 4)
knitr::opts_chunk$set(echo = TRUE)
```
# Basic R
## Cast dates
```{r, echo = TRUE}
start_date <- "2017-01-01"
end_date <- "2019-12-31"
f1<-function(d2, d1){
  n_weeks <-  floor(as.numeric(difftime(d2, d1, units="weeks")))
}
f2<-function(d2, d1){
  n_weeks <- floor(as.numeric(difftime(as.Date(d2)
    , as.Date(d1), units = "weeks")))
}
m1<-microbenchmark(
  Nocast = f1(end_date, start_date),
  Cast = f2(end_date, start_date),
  times = 1000
)
print(m1)
```

```{r, warning = FALSE}
fbox_plot(m1, "microseconds")
```

## Explicit vector length vector("type", length) is faster than an empty vector c()
```{r, echo = TRUE}
no_size <- function (n){
  x <- c()
  for (i in seq(n)) {
    x <- c(x, i)
  }
}
explicit_size <- function (n){
  x <- vector("integer", n)
  for (i in seq(n)) {
    x[i] <- i
  }
}
m3 <- microbenchmark(
  no_size = no_size(1e4),
  explicit_size = explicit_size(1e4),
  times = 10
)
print(m3)
```

```{r, warning = FALSE}
fbox_plot(m3, "microseconds")
```

## which function is slow for some simple situations

```{r, echo = TRUE}
vector <- runif(1e8)
w1 <- function(x){
  d <- length(which(x > .5))
}
w2 <- function(x){
  d <- sum(x > .5)
}

m4 <- microbenchmark(
  which = w1(vector),
  nowhich = w2(vector),
  times = 10
)
print(m4)
```

```{r, warning = FALSE}
fbox_plot(m4, "miliseconds")
```

## Column operation is faster than row operation

```{r, echo = TRUE}
n <- 1e4
dt <- data.table(
  a = seq(n), b = runif(n)
)
v1 <- function(dt){
  d <- mean(dt[dt$b > .5, ]$a)
}
v2 <- function(dt){
  d <- mean(dt$a[dt$b > .5])
}
m5 <- microbenchmark(
  row_operation = v1(dt),
  column_operation = v2(dt),
  times = 10
)
print(m5)
```

```{r, warning = FALSE}
fbox_plot(m5, "microseconds")
```

## Sequences function safer than 1:n

The function seq prevents when the second part of the 1:x is zero

```{r, echo = TRUE}
num <- 1e7
s1 <- function(num){
  d <- mean(1:num)
}
s2 <- function(num){
  d <- mean(seq(num))
}
m6<-microbenchmark(
  noseq = s1(num),
  seq = s2(num),
  times = 30
)
print(m6)
```

```{r, warning = FALSE}
fbox_plot(m6, "miliseconds")
```

## paste0 is faster than glue

```{r, echo = TRUE}
large_dataset <- data.table(
  id = 1:1000000,
  value = sample(letters, 1000000, replace = TRUE)
)
a1 <- function(x){
  d <- x |> mutate(code = paste0(id, "_", value))
}
a2 <- function(x){
  d <- x |> mutate(code = glue("{id}_{value}"))
}
m7 <- microbenchmark(
  with_paste = a1(large_dataset),
  with_glue = a2(large_dataset),
  times = 20
)
print(m7)
```

```{r, warning = FALSE}
fbox_plot(m7, "miliseconds")
```

## for loop vs lapply

```{r, echo = TRUE}

# Create a large list
big_list <- replicate(1e5, rnorm(10), simplify = FALSE)

m8 <- microbenchmark(
  lapply = lapply(big_list, mean),
  for_loop = {
    result <- list()
    for (i in seq_along(big_list)) {
      result[[i]] <- mean(big_list[[i]])
    }
  },
  times = 10
)

print(m8)
```

```{r, warning = FALSE}
fbox_plot(m8, "miliseconds")
```
# data.table package functions
## Date vs IDate

```{r, echo = TRUE}
dt <- data.table(
  Date = as.Date('2023-01-01') + 0:99999,
  iDate = as.IDate('2023-01-01') + 0:99999,
  Value = rnorm(100000)
)

nd1 <- as.Date('2023-01-01')
nd2 <- as.Date('2023-01-10')
id1 <- as.IDate('2023-01-01')
id2 <- as.IDate('2023-01-10')

date_between_operation <- function(nd1, nd2) {
  dt |> filter(Date >= nd1 & Date <= nd2)
}
idate_between_operation <- function(id1, id2) {
  dt |> _[data.table::between(iDate, id1, id2)]
}

m9 <- microbenchmark(
  Date = date_between_operation(nd1, nd2),
  iDate = idate_between_operation(id1, id2),
  times = 200L
)
print(m9)
```

```{r, warning = FALSE}
fbox_plot(m9, "miliseconds")
```

## Base R switch vs Dplyr case_when (for simple tasks)

```{r, echo = TRUE}
switch_function <- function(x) {
  switch(x,
         "a" = "apple",
         "b" = "banana",
         "c" = "cherry",
         "default")
}
case_when_function <- function(x) {
  case_when(
    x == "a" ~ "apple",
    x == "b" ~ "banana",
    x == "c" ~ "cherry",
    TRUE ~ "default"
  )
}
# Create a vector of test values
test_values <- sample(c("a", "b", "c", "d"), 1000, replace = TRUE)
m10 <- microbenchmark(
  switch = sapply(test_values, switch_function),
  case_when = sapply(test_values, case_when_function),
  times = 200L
)
print(m10)
```

```{r, warning = FALSE}
fbox_plot(m10, "microseconds")
```

## data.table fcase vs Dplyr case_when

```{r, echo = TRUE}
set.seed(123)
n <- 1e6
data <- data.table(
  id = seq(n),
  value = sample(seq(100), n, replace = TRUE)
)

casewhenf <- function(data){
  df <- data |> 
    mutate(category = case_when(
      value <= 20 ~ "Low",
      value <= 70 ~ "Medium",
      value > 70 ~ "High"))
}
fcasef <- function(data){
  df <- data |> 
    mutate(category = fcase(
      value <= 20, "Low",
      value <= 70, "Medium",
      value > 70, "High"))
}
m11 <- microbenchmark(
  case_when = casewhenf(data),
  fcase = fcasef(data),
  times = 20
)
print(m11)
```

```{r, warning = FALSE}
fbox_plot(m11, "miliseconds")
```

## data.table fcoalesce vs tidyr replace_na

```{r, echo = TRUE}
set.seed(123)
DT <- data.table(
  ID = 1:1e6,
  Value1 = sample(c(NA, 1:100), 1e6, replace = TRUE),
  Value2 = sample(c(NA, 101:200), 1e6, replace = TRUE)
)

# Define the functions
replace_na_f <- function(data){
  DF <- data |> 
    mutate(Value1 = replace_na(Value1, 0),
           Value2 = replace_na(Value2, 0)) |> 
    as.data.table()
}
fcoalesce_f <- function(data){
  DF <- data |> 
    mutate(Value1 = fcoalesce(Value1, 0L),
           Value2 = fcoalesce(Value2, 0L))
}
m12 <- microbenchmark(
  treplace_na = replace_na_f(DT),
  tfcoalesce = fcoalesce_f(DT),
  times = 20
)
print(m12)
```

```{r, warning = FALSE}
fbox_plot(m12, "miliseconds")
```

## data.table notation vs dplyr notation

```{r, echo = TRUE}
dt <- data.table(field_name = c("argentina.blue.man.watch", 
                                "brazil.red.woman.shoes", 
                                "canada.green.kid.hat", 
                                "denmark.red.man.shirt"))

# Filter rows where 'field_name' does not contain 'red'
dtnot <- function(data){
  filtered_dt <- data |> _[!grepl("red", field_name)]
}
dplyrnot <- function(data){
  filtered_dt <- data |> filter(!grepl("red", field_name))
}

m13 <- microbenchmark(
  data_table_not = dtnot(dt),
  dplyrnot = dplyrnot(dt),
  times = 100
)
print(m13)
```

```{r, warning = FALSE}
fbox_plot(m13, "microseconds")
```

## data.table melt vs tidyr pivot_longer

```{r, echo = TRUE}
large_data <- data.table(
  id = 1:100000,
  var1 = rnorm(100000),
  var2 = rnorm(100000),
  var3 = rnorm(100000),
  var4 = rnorm(100000)
)
# Benchmarking
m14 <- microbenchmark(
  tidyr_pivot_longer = {
    long_data_tidyr <- pivot_longer(large_data, cols = starts_with("var"), 
                                    names_to = "variable", values_to = "value")
  },
  data_table_melt = {
    long_data_dt <- melt(large_data, id.vars = "id", variable.name = "variable", 
                         value.name = "value")
  },
  times = 10
)

print(m14)
```

```{r, warning = FALSE}
fbox_plot(m14, "microseconds")
```

## data.table CJ vs tidyr expand_grid

```{r, echo = TRUE}
vec1 <- seq(1000)
vec2 <- seq(1000)

# Define functions to be benchmarked
expand_grid_func <- function() {
  return(expand_grid(vec1, vec2))
}

CJ_func <- function() {
  return(CJ(vec1, vec2))
}

# Perform benchmarking
m15 <- microbenchmark(
  expand_grid = expand_grid_func(),
  CJ = CJ_func(),
  times = 10
)

print(m15)
```

```{r, warning = FALSE}
fbox_plot(m15, "microseconds")
```

## data.table rbindlist vs R rbind

```{r, echo = TRUE}
# Sample data
size = 1e4
set.seed(44)
df_list <- replicate(50, data.table(id = sample(seq(size), size, replace = T),
                                    value = rnorm(size)), simplify = F)

simple_bind <- function(list_of_dfs){
  do.call(rbind, list_of_dfs)
}

dplyr_bind <- function(list_of_dfs){
  bind_rows(list_of_dfs)
}

dt_bind <- function(list_of_dfs){
  rbindlist(list_of_dfs, fill = F)
}

# Benchmark both methods
m16 <- microbenchmark(
  dt_ver = dt_bind(df_list),
  simple = simple_bind(df_list),
  dplyr_ver = dplyr_bind(df_list),
  times = 30
)

print(m16)
```

```{r, warning = FALSE}
fbox_plot(m16, "microseconds")
```

## stringr word vs tidyr separate vs data.table tstrsplit

```{r, echo = TRUE}
set.seed(123)
n <- 1e4
df <- data.table(text = paste("word1", "word2", "word3", "word4", "word5", sep = "."), stringsAsFactors = F)
df <- df[rep(1, n), , drop = F]

# Using tidyr::separate
separate_words <- function() {
  df |> 
    separate(text, into = c("w1", "w2", "w3", "w4", "w5"), sep = "\\.", remove = F) |> 
    select(-c(w1, w2, w4))
}

# Using stringr::word
stringr_words <- function() {
  df |> 
    mutate(
      w3 = word(text, 3, sep = fixed(".")),
      w5 = word(text, 5, sep = fixed("."))
    )
}

datatable_words <- function() {
  df |> _[, c("w3", "w5") := tstrsplit(text, "\\.")[c(3, 5)]]
}

m17 <- microbenchmark(
  separate = separate_words(),
  stringr = stringr_words(),
  dt = datatable_words(),
  times = 10
)

print(m17)
```

```{r, warning = FALSE}
fbox_plot(m17, "miliseconds")
```

## data.table na_omit vs dplyr drop_na

```{r, echo = TRUE}
# Sample data
set.seed(123)
n <- 1e6
df <- data.table(
  x = rnorm(n),
  y = sample(c(NA, 1:100), n, replace = TRUE),
  z = sample(c(NA, letters), n, replace = TRUE),
  stringsAsFactors = F
)

# Benchmark both methods
m18 <- microbenchmark(
  dplyr_drop_na = {
    df |> drop_na()
  },
  data_table_na_omit = {
    dt |> na.omit()
  },
  times = 10
)

print(m18)
```

```{r, warning = FALSE}
fbox_plot(m18, "microseconds")
```
# Parallel processing
## lapply vs parallel mclapply

```{r, echo = TRUE}
# Sample data
set.seed(123)

size = 1e4
n_cores = parallelly::availableCores()

df_list <- replicate(100, data.table(id = sample(seq(size), size, replace = T),
                                    value = rnorm(size)), simplify = F)
extra_df <- data.table(id = sample(seq(size), size, replace = T), 
                       extra_value = runif(size))

# Sequential join
sequential_join <- function() {
  lapply(df_list, function(df) {
    merge(df, extra_df, by = "id", allow.cartesian = T)
  })
}

# Parallel join using mclapply
parallel_join <- function() {
  mclapply(df_list, function(df) {
    merge(df, extra_df, by = "id", allow.cartesian = T)
  }, mc.cores = n_cores, mc.silent = T, mc.cleanup = T)
}

# Benchmark both methods
m19 <- microbenchmark(
  sequential = sequential_join(),
  parallel = parallel_join(),
  times = 10
)

print(m19)
```

```{r, warning = FALSE}
fbox_plot(m19, "miliseconds")
```
# dtplyr
This is another alternative (You need to install this package)

```{r, echo = TRUE}
set.seed(123)
n <- 1e7
df <- data.table(
  group1 = sample(LETTERS[1:10], n, replace = TRUE),
  group2 = sample(letters[1:5], n, replace = TRUE),
  value1 = rnorm(n),
  value2 = runif(n, 1, 100)
)

m21 <- microbenchmark(
  basic_way = {
    dplyr <- df |> 
      filter(value1 > 0) |> 
      mutate(ratio = value1 / value2) |> 
      summarize(
        mean_val1 = mean(value1),
        sd_val1 = sd(value1),
        median_val2 = median(value2),
        max_ratio = max(ratio), .by = c("group1", "group2")) |> 
      as.data.table()
  },
  dtplyr_way = {
    dtplyr = df |> 
      lazy_dt() |> 
      filter(value1 > 0) |> 
      mutate(ratio = value1 / value2) |> 
      summarize(
        mean_val1 = mean(value1),
        sd_val1 = sd(value1),
        median_val2 = median(value2),
        max_ratio = max(ratio), .by = c("group1", "group2")) |> 
      as.data.table()
  },
  times = 5
)

print(m21)
```

```{r, warning = FALSE}
fbox_plot(m21, "miliseconds")
```

# Duckdb
## Dockdb files vs parquet

```{r, warning = FALSE}
with_parquet <- function(){
  fp_data <- "/conf/posit_azure_logs/data"
  
  data_1 <- open_dataset(file.path(glue::glue("{fp_data}/golden_data_in_progress"))) |>
    select(
      date, hours, time,
      ALL_WIP_CP_day_session, ALL_WIP_CP_night_session,
      ALL_WIP_BP_day_session, ALL_WIP_BP_night_session,
      ALL_WIP_CP_DS_mem_limit, ALL_WIP_CP_NS_mem_limit,
      ALL_WIP_BP_DS_mem_limit, ALL_WIP_BP_NS_mem_limit,
      ALL_WIP_CP_DS_mem_request, ALL_WIP_CP_NS_mem_request,
      ALL_WIP_BP_DS_mem_request, ALL_WIP_BP_NS_mem_request,
      ALL_WIP_CP_DS_mem_max, ALL_WIP_CP_NS_mem_max,
      ALL_WIP_BP_DS_mem_max, ALL_WIP_BP_NS_mem_max,
      ALL_WIP_CP_node_total, ALL_WIP_BP_node_total
    ) |>
    mutate(
      computepool_node_mem = ALL_WIP_CP_node_total * (160 * 1024),
      bigpool_node_mem = ALL_WIP_BP_node_total * (256 * 1024),
      ALL_WIP_day_session = ALL_WIP_CP_day_session + ALL_WIP_BP_day_session,
      ALL_WIP_night_session = ALL_WIP_CP_night_session + ALL_WIP_BP_night_session,
      ALL_WIP_node_total = ALL_WIP_CP_node_total + ALL_WIP_BP_node_total,
      total_mem_limit = ALL_WIP_CP_DS_mem_limit + ALL_WIP_CP_NS_mem_limit + ALL_WIP_BP_DS_mem_limit + ALL_WIP_BP_NS_mem_limit,
      total_mem_request = ALL_WIP_CP_DS_mem_request + ALL_WIP_CP_NS_mem_request + ALL_WIP_BP_DS_mem_request + ALL_WIP_BP_NS_mem_request,
      total_mem_max = ALL_WIP_CP_DS_mem_max + ALL_WIP_CP_NS_mem_max + ALL_WIP_BP_DS_mem_max + ALL_WIP_BP_NS_mem_max,
      total_node_mem = computepool_node_mem + bigpool_node_mem,
      average_session_per_node = ifelse(ALL_WIP_node_total != 0,
                                         (ALL_WIP_day_session + ALL_WIP_night_session) / ALL_WIP_node_total, 0)
    ) |>
    collect() |>
    as.data.table()
}

with_duckfile <- function(){
  file.copy("/conf/posit_azure_logs/gatzos01/gd_inprogress.duckdb", "gd_inprogress.duckdb")
  
  data_2 <- res_duckdb_sql <- dbGetQuery(
    conn = dbConnect(duckdb::duckdb(), dbdir = "./gd_inprogress.duckdb"),
    statement = glue("select date, hours, time,
        ALL_WIP_CP_day_session, ALL_WIP_CP_night_session,
        ALL_WIP_BP_day_session, ALL_WIP_BP_night_session,
        ALL_WIP_CP_DS_mem_limit, ALL_WIP_CP_NS_mem_limit,
        ALL_WIP_BP_DS_mem_limit, ALL_WIP_BP_NS_mem_limit,
        ALL_WIP_CP_DS_mem_request, ALL_WIP_CP_NS_mem_request,
        ALL_WIP_BP_DS_mem_request, ALL_WIP_BP_NS_mem_request,
        ALL_WIP_CP_DS_mem_max, ALL_WIP_CP_NS_mem_max,
        ALL_WIP_BP_DS_mem_max, ALL_WIP_BP_NS_mem_max,
        ALL_WIP_CP_node_total, ALL_WIP_BP_node_total,
        ALL_WIP_CP_node_total * 160 * 1024 as computepool_node_mem,
        ALL_WIP_BP_node_total * 256 * 1024 as bigpool_node_mem,
        ALL_WIP_CP_day_session + ALL_WIP_BP_day_session as ALL_WIP_day_session,
        ALL_WIP_CP_night_session + ALL_WIP_BP_night_session as ALL_WIP_night_session,
        ALL_WIP_CP_node_total + ALL_WIP_BP_node_total as ALL_WIP_node_total,
        ALL_WIP_CP_DS_mem_limit + ALL_WIP_CP_NS_mem_limit + ALL_WIP_BP_DS_mem_limit + ALL_WIP_BP_NS_mem_limit as total_mem_limit,
        ALL_WIP_CP_DS_mem_request + ALL_WIP_CP_NS_mem_request + ALL_WIP_BP_DS_mem_request + ALL_WIP_BP_NS_mem_request as total_mem_request,
        ALL_WIP_CP_DS_mem_max + ALL_WIP_CP_NS_mem_max + ALL_WIP_BP_DS_mem_max + ALL_WIP_BP_NS_mem_max as total_mem_max,
        computepool_node_mem + bigpool_node_mem as total_node_mem,
        CASE 
          WHEN ALL_WIP_node_total != 0 THEN (ALL_WIP_day_session + ALL_WIP_night_session) / ALL_WIP_node_total
          ELSE 0
        END AS average_session_per_node
      from gdinprog"),
    immediate = TRUE) |> 
    as.data.table()
  
  file.remove("./gd_inprogress.duckdb")
}

m22 <- microbenchmark(
  with_parquet = with_parquet(),
  with_duckfile = with_duckfile(),
  times = 3
)

print(m22)
```

```{r, warning = FALSE}
fbox_plot(m22, "miliseconds")
```
